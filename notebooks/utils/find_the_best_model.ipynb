{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The process of find the best ML model\n",
    "\n",
    "This notebook is part of **Story 2.1: ML Model Training and Persistence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1782 rows from 'matches' table\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas.io.sql import DatabaseError\n",
    "\n",
    "# Path to database\n",
    "db_path = '../../football.db'\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    raise FileNotFoundError(f\"Database '{db_path}' not found. Run db_setup.py first.\")\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_sql_query(\"SELECT * FROM matches\", conn)\n",
    "    print(f\"✅ Loaded {len(df_raw)} rows from 'matches' table\")\n",
    "except DatabaseError as e:\n",
    "    df_raw = pd.DataFrame()\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering 1: add column 'Season' (for time series training-testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_season(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    def get_season(date):\n",
    "        if pd.Timestamp('2024-07-26') <= date <= pd.Timestamp('2025-05-29'):\n",
    "            return '2024/2025'\n",
    "        elif pd.Timestamp('2023-07-28') <= date <= pd.Timestamp('2024-06-02'):\n",
    "            return '2023/2024'\n",
    "        elif pd.Timestamp('2022-07-22') <= date <= pd.Timestamp('2023-04-23'):\n",
    "            return '2022/2023'\n",
    "        elif pd.Timestamp('2021-07-23') <= date <= pd.Timestamp('2022-04-10'):\n",
    "            return '2021/2022'\n",
    "        elif pd.Timestamp('2020-08-08') <= date <= pd.Timestamp('2021-04-18'):\n",
    "            return '2020/2021'\n",
    "        elif pd.Timestamp('2019-07-26') <= date <= pd.Timestamp('2020-03-07'):\n",
    "            return '2019/2020'\n",
    "        else:\n",
    "            return '2025/2026'\n",
    "\n",
    "    df['Season'] = df['Date'].apply(get_season)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering 2: add rolling goal-related overall performance columns(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_rolling_perf(df, rolling_window):\n",
    "    # Copy and sort dataset chronologically\n",
    "    df_raw = df.copy()\n",
    "    df_raw = df_raw.sort_values('Date').reset_index(drop=True)\n",
    "    df_raw['rowid'] = df_raw.index  # Unique ID to merge later\n",
    "\n",
    "    # Home team features\n",
    "    home_df = df_raw[['rowid', 'Date', 'HomeTeam', 'FTR', 'HS', 'HST', 'HC', 'HF', 'HY', 'HR', 'FTHG', 'FTAG']].copy()\n",
    "    home_df['team'] = home_df['HomeTeam']\n",
    "    home_df['side'] = 'home'\n",
    "    home_df['points'] = home_df['FTR'].map({'H': 3, 'D': 1, 'A': 0})\n",
    "    home_df = home_df.rename(columns={\n",
    "        'HS': 'shots', 'HST': 'shots_on_target',\n",
    "        'HC': 'corners', 'HF': 'fouls',\n",
    "        'HY': 'yellow_cards', 'HR': 'red_cards',\n",
    "        'FTHG': 'goals_scored', 'FTAG': 'goals_conceded'\n",
    "    })\n",
    "\n",
    "    # Away team features\n",
    "    away_df = df_raw[['rowid', 'Date', 'AwayTeam', 'FTR', 'AS', 'AST', 'AC', 'AF', 'AY', 'AR', 'FTAG', 'FTHG']].copy()\n",
    "    away_df['team'] = away_df['AwayTeam']\n",
    "    away_df['side'] = 'away'\n",
    "    away_df['points'] = away_df['FTR'].map({'A': 3, 'D': 1, 'H': 0})\n",
    "    away_df = away_df.rename(columns={\n",
    "        'AS': 'shots', 'AST': 'shots_on_target',\n",
    "        'AC': 'corners', 'AF': 'fouls',\n",
    "        'AY': 'yellow_cards', 'AR': 'red_cards',\n",
    "        'FTAG': 'goals_scored', 'FTHG': 'goals_conceded'\n",
    "    })\n",
    "\n",
    "    # Combine and sort\n",
    "    team_games = pd.concat([\n",
    "        home_df[['rowid', 'Date', 'team', 'side', 'points', 'shots', 'shots_on_target',\n",
    "                 'corners', 'fouls', 'yellow_cards', 'red_cards', 'goals_scored', 'goals_conceded']],\n",
    "        away_df[['rowid', 'Date', 'team', 'side', 'points', 'shots', 'shots_on_target',\n",
    "                 'corners', 'fouls', 'yellow_cards', 'red_cards', 'goals_scored', 'goals_conceded']]\n",
    "    ])\n",
    "    team_games = team_games.sort_values(by=['team', 'Date'])\n",
    "\n",
    "    # Weight setup\n",
    "    weights = np.arange(1, rolling_window + 1)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    def weighted_avg(series, weights_array):\n",
    "        w = weights_array[-len(series):]\n",
    "        return np.dot(series, w)\n",
    "\n",
    "    # Form Score (weighted Points)\n",
    "    team_games['form_score'] = (\n",
    "        team_games.groupby('team')['points']\n",
    "        .apply(lambda x: x.shift(1).rolling(window=rolling_window, min_periods=rolling_window)\n",
    "               .apply(lambda y: weighted_avg(y, weights), raw=True))\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Other stats (weighted Averages)\n",
    "    for col in ['shots', 'shots_on_target', 'corners', 'fouls',\n",
    "                'yellow_cards', 'red_cards', 'goals_scored', 'goals_conceded']:\n",
    "        team_games[f'avg_{col}'] = (\n",
    "            team_games.groupby('team')[col]\n",
    "            .apply(lambda x: x.shift(1).rolling(window=rolling_window, min_periods=rolling_window)\n",
    "                   .apply(lambda y: weighted_avg(y, weights), raw=True))\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    # Split home/away features\n",
    "    home_features = team_games[team_games['side'] == 'home'].copy()\n",
    "    away_features = team_games[team_games['side'] == 'away'].copy()\n",
    "\n",
    "    # Merge back to original data\n",
    "    df_enriched = df_raw.merge(home_features[[\n",
    "        'rowid', 'form_score', 'avg_shots', 'avg_shots_on_target', 'avg_corners',\n",
    "        'avg_fouls', 'avg_yellow_cards', 'avg_red_cards',\n",
    "        'avg_goals_scored', 'avg_goals_conceded'\n",
    "    ]], on='rowid', how='left').rename(columns={\n",
    "        'form_score': 'HomeTeam_FormScore',\n",
    "        'avg_shots': 'HomeTeam_AvgShots',\n",
    "        'avg_shots_on_target': 'HomeTeam_AvgShotsOnTarget',\n",
    "        'avg_corners': 'HomeTeam_AvgCorners',\n",
    "        'avg_fouls': 'HomeTeam_AvgFouls',\n",
    "        'avg_yellow_cards': 'HomeTeam_AvgYellowCards',\n",
    "        'avg_red_cards': 'HomeTeam_AvgRedCards',\n",
    "        'avg_goals_scored': 'HomeTeam_AvgGoalsScored',\n",
    "        'avg_goals_conceded': 'HomeTeam_AvgGoalsConceded'\n",
    "    })\n",
    "\n",
    "    df_enriched = df_enriched.merge(away_features[[\n",
    "        'rowid', 'form_score', 'avg_shots', 'avg_shots_on_target', 'avg_corners',\n",
    "        'avg_fouls', 'avg_yellow_cards', 'avg_red_cards',\n",
    "        'avg_goals_scored', 'avg_goals_conceded'\n",
    "    ]], on='rowid', how='left').rename(columns={\n",
    "        'form_score': 'AwayTeam_FormScore',\n",
    "        'avg_shots': 'AwayTeam_AvgShots',\n",
    "        'avg_shots_on_target': 'AwayTeam_AvgShotsOnTarget',\n",
    "        'avg_corners': 'AwayTeam_AvgCorners',\n",
    "        'avg_fouls': 'AwayTeam_AvgFouls',\n",
    "        'avg_yellow_cards': 'AwayTeam_AvgYellowCards',\n",
    "        'avg_red_cards': 'AwayTeam_AvgRedCards',\n",
    "        'avg_goals_scored': 'AwayTeam_AvgGoalsScored',\n",
    "        'avg_goals_conceded': 'AwayTeam_AvgGoalsConceded'\n",
    "    })\n",
    "\n",
    "    return df_enriched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train and test sets: ##\n",
    "*Note: Apply mannual preprocessing (i.e., feature engineering1 + feature engineering2) to dataset before feeding into sklearn/H2o/NN pipelines.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_sets(df_raw, rolling_window, selected_features, test_seasons, train_seasons):\n",
    "    # Add 'Season' and rolling performance cols to dataset, drop null values\n",
    "    df = add_season(df_raw)\n",
    "    df = add_rolling_perf(df, rolling_window=rolling_window)\n",
    "    if selected_features == 'all':\n",
    "        selected_features = ['HomeTeam', 'AwayTeam'] + df.iloc[:, -18:].columns.to_list() + ['B365H', 'B365D', 'B365A']\n",
    "    elif selected_features == 'all_rolling_perf':\n",
    "        selected_features = ['HomeTeam', 'AwayTeam'] + df.iloc[:, -18:].columns.to_list()\n",
    "    df = df[selected_features + ['FTR'] +['Season']].dropna() #dropna is optional, if not, will be imputed by mean in later sklearn pipelines\n",
    "\n",
    "    # Prepare train and test subsets\n",
    "    train_df = df[df['Season'].isin(train_seasons)]\n",
    "    train_df = train_df[selected_features + ['FTR']]\n",
    "    test_df = df[df['Season'].isin(test_seasons)]\n",
    "    test_df = test_df[selected_features + ['FTR']]\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['FTR']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test = test_df['FTR']\n",
    "\n",
    "    return train_df, test_df, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best single conventional ML model in sklearn\n",
    "*Loop all combinations of features & model*\n",
    "\n",
    "*Settings: train on 2019-2024 seasons, test on 2024-2025 season.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import softmax\n",
    "import itertools\n",
    "\n",
    "# Loop combination of features and model to find the best model:\n",
    "feature_groups = [['B365H', 'B365D', 'B365A'], # odds, mannually chosed\n",
    "                     ['HomeTeam_FormScore', 'AwayTeam_FormScore'],\n",
    "                     ['HomeTeam_AvgShots', 'AwayTeam_AvgShots'],\n",
    "                     ['HomeTeam_AvgShotsOnTarget', 'AwayTeam_AvgShotsOnTarget'],\n",
    "                     ['HomeTeam_AvgCorners', 'AwayTeam_AvgCorners'],\n",
    "                     ['HomeTeam_AvgFouls', 'AwayTeam_AvgFouls'],\n",
    "                     ['HomeTeam_AvgYellowCards', 'AwayTeam_AvgYellowCards'],\n",
    "                     ['HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards'],\n",
    "                     ['HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored'],\n",
    "                     ['HomeTeam_AvgGoalsConceded', 'AwayTeam_AvgGoalsConceded']]\n",
    "\n",
    "metric_ultra_max = 0\n",
    "model_ultra_max = \"Init\"\n",
    "features_ultra_max = []\n",
    "\n",
    "for r in range(1, len(feature_groups) + 1):\n",
    "    for combo in itertools.combinations(feature_groups, r):\n",
    "        # Prepare selected features\n",
    "        selected_features = ['HomeTeam', 'AwayTeam'] + [feature for group in combo for feature in group]\n",
    "        print(f\"📂 Selected features: {selected_features}\")\n",
    "\n",
    "        # Prepare train and test subsets\n",
    "        _, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "\n",
    "        # Make clear cat and num features in X for later sklearn pipeline\n",
    "        categorical_features = ['HomeTeam', 'AwayTeam']\n",
    "        numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "        # Prepare sklearn pipelines.\n",
    "        # Note that most models support one-hot, for models do not supporting sparse matrix (HistGradientBoosting, NaiveBayes, QDA), use ordinal encoding instead\n",
    "        # Note that imputer is useless if previous already dropna. Scaler is optional, we just applied it here.\n",
    "        numeric_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        # optional: PCA (proven to be useless, discard :( )\n",
    "        # numeric_pipeline = numeric_pipeline = Pipeline([\n",
    "        #     ('imputer', SimpleImputer(strategy='mean')),\n",
    "        #     ('scaler', StandardScaler()),\n",
    "        #     ('pca', PCA(n_components=0.95))\n",
    "        # ])\n",
    "        categorical_pipeline1 = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        categorical_pipeline2 = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ])\n",
    "        preprocessor1 = ColumnTransformer([\n",
    "            ('num', numeric_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline1, categorical_features)\n",
    "        ])\n",
    "        preprocessor2 = ColumnTransformer([\n",
    "            ('num', numeric_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline2, categorical_features)\n",
    "        ])\n",
    "        models_and_preprocessors = {\n",
    "            \"RandomForest\": [RandomForestClassifier(random_state=42), preprocessor1],\n",
    "            \"LogisticRegression\": [LogisticRegression(max_iter=1000, random_state=42), preprocessor1],\n",
    "            \"PassiveAggressiveClassifier\": [PassiveAggressiveClassifier(max_iter=1000, random_state=42), preprocessor1],\n",
    "            \"RidgeClassifier\": [RidgeClassifier(max_iter=1000), preprocessor1],\n",
    "            \"KNN\": [KNeighborsClassifier(), preprocessor1],\n",
    "            \"SVC\": [SVC(random_state=42), preprocessor1],\n",
    "            \"DecisionTree\": [DecisionTreeClassifier(random_state=42), preprocessor1],\n",
    "            \"GradientBoosting\": [GradientBoostingClassifier(random_state=42), preprocessor1],\n",
    "            \"HistGradientBoosting\": [HistGradientBoostingClassifier(random_state=42), preprocessor2],\n",
    "            \"AdaBoost\": [AdaBoostClassifier(random_state=42), preprocessor1],\n",
    "            \"ExtraTrees\": [ExtraTreesClassifier(random_state=42), preprocessor1],\n",
    "            \"NaiveBayes\": [GaussianNB(), preprocessor2],\n",
    "            \"MLP\": [MLPClassifier(max_iter=1000, random_state=42), preprocessor1],\n",
    "            \"QDA\": [QuadraticDiscriminantAnalysis(), preprocessor2],\n",
    "            \"LDA\": [LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto'), preprocessor2]\n",
    "        }\n",
    "\n",
    "        # Loop to train in different models and display selected features and accuracy\n",
    "        acc_max = 0\n",
    "        model_max = \"Init\"\n",
    "        for name, model_and_proprocessor in models_and_preprocessors.items():\n",
    "            model_pipeline = Pipeline([\n",
    "                ('preprocess', model_and_proprocessor[1]),\n",
    "                ('classifier', model_and_proprocessor[0])\n",
    "            ])\n",
    "            model_pipeline.fit(X_train, y_train)\n",
    "            # Option1: using acc as metric\n",
    "            y_pred = model_pipeline.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            print(f\"🎯 {name} Accuracy: {acc:.4f}\")\n",
    "            if acc > acc_max:\n",
    "                acc_max = acc\n",
    "                model_max = name\n",
    "            # # Option2: using broer score as metric\n",
    "            # if name == \"\":\n",
    "\n",
    "        \n",
    "        # Update the best model in this selected features\n",
    "        if acc_max > metric_ultra_max:\n",
    "            metric_ultra_max = acc_max\n",
    "            model_ultra_max = model_max\n",
    "            features_ultra_max = selected_features\n",
    "        \n",
    "    print(f\"The best model until now is {model_ultra_max}.\\nThe accuracy is {metric_ultra_max}.\\nUsed features: {features_ultra_max}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "\n",
    "1. When use **accuracy** as metric:\n",
    "\n",
    "   The best model is **LDA** with **05404** accuracy using the feature combination: ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored'].\n",
    "\n",
    "   The best model without betting features is **RidgeClassifier** with **0.5292** accuracy using the feature combination: ['HomeTeam', 'AwayTeam', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgShots', 'AwayTeam_AvgShots', 'HomeTeam_AvgFouls', 'AwayTeam_AvgFouls', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards']\n",
    "\n",
    "   See **sklearn_model_acc.txt** the records of all models' accuracy.\n",
    "\n",
    "2. When use **broer score** as metric:\n",
    "\n",
    "   See **sklearn_model_broer.txt** the records of all models' broer score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a way to improve the chosed model (tuning hyperparams?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Check and tune LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Check LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import pipeline_LDA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Check the draw of LDA\n",
    "print(\"😀Train LDA on 2019-2024, test on 2024-2025, check the test probabilities of Draw\")\n",
    "selected_features = ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored']\n",
    "_, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "pipe_LDA = pipeline_LDA()\n",
    "pipe_LDA.fit(X_train, y_train)\n",
    "\n",
    "probs = pipe_LDA.predict_proba(X_test)\n",
    "label_to_index = {label: idx for idx, label in enumerate(pipe_LDA.classes_)}\n",
    "probs_H = probs[:, label_to_index['H']]\n",
    "probs_D = probs[:, label_to_index['D']]\n",
    "probs_A = probs[:, label_to_index['A']]\n",
    "results_df = X_test.copy()\n",
    "results_df['Prob_H'] = probs_H\n",
    "results_df['Prob_D'] = probs_D\n",
    "results_df['Prob_A'] = probs_A\n",
    "display_cols = ['HomeTeam', 'AwayTeam', 'Prob_H', 'Prob_D', 'Prob_A']\n",
    "print(results_df[display_cols].to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "y_pred = pipe_LDA.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=pipe_LDA.classes_))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"🎯 Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: the draw is only predicted for 3 times, although the acc of predicted is high. Let's add Parity features and threshold tuning to solve this problem.\n",
    "\n",
    "*Class 'A' (Away Win): Precision: 0.44 → When the model predicts \"A\", it's right 44% of the time. Recall: 0.65 → Out of all actual \"A\" games, it correctly identified 65%.*\n",
    "\n",
    "*Class 'D' (Draw): Precision: 1.00 → The model only predicted \"D\" when it was correct — but this is misleading. Recall: 0.05 → It caught only 5% of actual draws! Terrible sensitivity.*\n",
    "\n",
    "*Class 'H' (Home Win): Precision: 0.59 → Not bad, 59% correct when predicting \"H\". Recall: 0.79 → It caught 79% of all actual home wins.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Tune with parity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "selected_features = ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored']\n",
    "_, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "X_train['FormDiff'] = abs(X_train['HomeTeam_FormScore'] - X_train['AwayTeam_FormScore'])\n",
    "X_test['FormDiff'] = abs(X_test['HomeTeam_FormScore'] - X_test['AwayTeam_FormScore'])\n",
    "X_train['RedcardsDiff'] = abs(X_train['HomeTeam_AvgRedCards'] - X_train['AwayTeam_AvgRedCards'])\n",
    "X_test['RedcardsDiff'] = abs(X_test['HomeTeam_AvgRedCards'] - X_test['AwayTeam_AvgRedCards'])\n",
    "X_train['GoalsscoredDiff'] = abs(X_train['HomeTeam_AvgGoalsScored'] - X_train['AwayTeam_AvgGoalsScored'])\n",
    "X_test['GoalsscoredDiff'] = abs(X_test['HomeTeam_AvgGoalsScored'] - X_test['AwayTeam_AvgGoalsScored'])\n",
    "\n",
    "added_feature_groups = [['FormDiff'], ['RedcardsDiff'],['GoalsscoredDiff']]\n",
    "\n",
    "for r in range(1, len(added_feature_groups) + 1):\n",
    "    for combo in itertools.combinations(added_feature_groups, r):\n",
    "        numerical_features = ['B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored'] + [feature for group in combo for feature in group]\n",
    "        print(f\"😀 numerical_features: {numerical_features}\")\n",
    "        pipe_LDA = Pipeline([\n",
    "            ('ColumnTransform', ColumnTransformer([\n",
    "                ('num', Pipeline([('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())]), numerical_features),\n",
    "                ('cat', Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))]), ['HomeTeam', 'AwayTeam'])\n",
    "            ])),\n",
    "            ('classifier', LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto'))\n",
    "        ])\n",
    "        pipe_LDA.fit(X_train, y_train)\n",
    "        probs = pipe_LDA.predict_proba(X_test)\n",
    "        labels = pipe_LDA.classes_\n",
    "        label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "        probs_H = probs[:, label_to_index['H']]\n",
    "        probs_D = probs[:, label_to_index['D']]\n",
    "        probs_A = probs[:, label_to_index['A']]\n",
    "        results_df = X_test.copy()\n",
    "        results_df['Prob_H'] = probs_H\n",
    "        results_df['Prob_D'] = probs_D\n",
    "        results_df['Prob_A'] = probs_A\n",
    "        display_cols = ['HomeTeam', 'AwayTeam', 'Prob_H', 'Prob_D', 'Prob_A']\n",
    "        print(results_df[display_cols].to_string(index=False, float_format='%.3f'))\n",
    "        y_pred = pipe_LDA.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"🎯 Accuracy: {acc:.4f}\")\n",
    "        print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: not big improvements, let's try threshold tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Tune with threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import pipeline_LDA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "selected_features = ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored']\n",
    "_, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "pipe_LDA = pipeline_LDA()\n",
    "pipe_LDA.fit(X_train, y_train)\n",
    "\n",
    "probs = pipe_LDA.predict_proba(X_test)\n",
    "labels = pipeline_LDA.classes_\n",
    "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "probs_H = probs[:, label_to_index['H']]\n",
    "probs_D = probs[:, label_to_index['D']]\n",
    "probs_A = probs[:, label_to_index['A']]\n",
    "results_df = X_test.copy()\n",
    "results_df['Prob_H'] = probs_H\n",
    "results_df['Prob_D'] = probs_D\n",
    "results_df['Prob_A'] = probs_A\n",
    "display_cols = ['HomeTeam', 'AwayTeam', 'Prob_H', 'Prob_D', 'Prob_A']\n",
    "print(results_df[display_cols].to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "idx = {label: i for i, label in enumerate(labels)}\n",
    "draw_threshold = 0.35\n",
    "y_pred_thresh = []\n",
    "for p in probs:\n",
    "    if p[label_to_index['D']] > draw_threshold:\n",
    "        y_pred_thresh.append('D')\n",
    "    else:\n",
    "        y_pred_thresh.append(labels[np.argmax(p)])\n",
    "\n",
    "y_pred = pipe_LDA.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"🎯 Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best mixed model in H2O AutoML\n",
    "*H2O automately selects features and decide mixed models*\n",
    "\n",
    "*Settings: train on 2019-2024 seasons, test on 2024-2025 season.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.automl import get_leaderboard\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initiate h2o with the train and test set\n",
    "h2o.init()\n",
    "train_df, test_df, _, _, _, _ = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=\"all\", test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "train_h2o = h2o.H2OFrame(train_df)\n",
    "test_h2o = h2o.H2OFrame(test_df)\n",
    "\n",
    "# Ensure H2o know the target is a classification problem\n",
    "target_col = 'FTR'\n",
    "train_h2o[target_col] = train_h2o[target_col].asfactor()\n",
    "test_h2o[target_col] = test_h2o[target_col].asfactor()\n",
    "\n",
    "# Ensure H2o know the categorial cols in predictor features is categorial\n",
    "for col in ['HomeTeam', 'AwayTeam']:\n",
    "    train_h2o[col] = train_h2o[col].asfactor()\n",
    "    test_h2o[col] = test_h2o[col].asfactor()\n",
    "\n",
    "# Set the param for automl and train it\n",
    "aml = H2OAutoML(\n",
    "    # max_models=50, # try maximum 50 models\n",
    "    max_runtime_secs=600, # limit total runtime to 600 seconds\n",
    "    balance_classes=True, # upsample the minority classes - \"Draw\"\n",
    "    sort_metric='logloss', # good for multi-class classification\n",
    "    nfolds=5, # use 5-fold cross-validation\n",
    "    stopping_metric='logloss', # early stopping based on log loss\n",
    "    stopping_rounds=30, # early stop after 30 rounds of no improvement\n",
    "    seed=42 # random state 42\n",
    ")\n",
    "aml.train(x=[col for col in train_h2o.columns if col != target_col], y=target_col, training_frame=train_h2o)\n",
    "\n",
    "# Get the best mixed model from the leaderboard\n",
    "lb = get_leaderboard(aml, extra_columns='ALL')\n",
    "top_models = lb.head(rows=1) # only need the top 1 model here\n",
    "model_id = top_models.as_data_frame().iloc[0]['model_id']\n",
    "model = h2o.get_model(model_id)\n",
    "\n",
    "# Show model and train-val info\n",
    "if model.algo == \"stackedensemble\":\n",
    "    print(f\"🖥️ {model.metalearner()}\")\n",
    "else:\n",
    "    print(f\"🖥️ {model.algo}\")\n",
    "    print(\"Info:\", model._model_json['output'])\n",
    "            \n",
    "# Display accuracy\n",
    "preds = model.predict(test_h2o).as_data_frame()['predict']\n",
    "true = test_h2o[target_col].as_data_frame()[target_col]\n",
    "acc = accuracy_score(true, preds)\n",
    "print(f\"🎯 Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "\n",
    "The best accuracy is **0.5037**, worse than single model, not adopted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try NN in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
