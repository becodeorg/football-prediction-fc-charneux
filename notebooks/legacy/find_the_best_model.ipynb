{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The process of find the best ML model\n",
    "\n",
    "This notebook is part of **Story 2.1: ML Model Training and Persistence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1782 rows from 'matches' table\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas.io.sql import DatabaseError\n",
    "\n",
    "# Path to database\n",
    "db_path = '../../football.db'\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    raise FileNotFoundError(f\"Database '{db_path}' not found. Run db_setup.py first.\")\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_sql_query(\"SELECT * FROM matches\", conn)\n",
    "    print(f\"✅ Loaded {len(df_raw)} rows from 'matches' table\")\n",
    "except DatabaseError as e:\n",
    "    df_raw = pd.DataFrame()\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering 1: add column 'Season' (for time series training-testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_season(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    def get_season(date):\n",
    "        if pd.Timestamp('2024-07-26') <= date <= pd.Timestamp('2025-05-29'):\n",
    "            return '2024/2025'\n",
    "        elif pd.Timestamp('2023-07-28') <= date <= pd.Timestamp('2024-06-02'):\n",
    "            return '2023/2024'\n",
    "        elif pd.Timestamp('2022-07-22') <= date <= pd.Timestamp('2023-04-23'):\n",
    "            return '2022/2023'\n",
    "        elif pd.Timestamp('2021-07-23') <= date <= pd.Timestamp('2022-04-10'):\n",
    "            return '2021/2022'\n",
    "        elif pd.Timestamp('2020-08-08') <= date <= pd.Timestamp('2021-04-18'):\n",
    "            return '2020/2021'\n",
    "        elif pd.Timestamp('2019-07-26') <= date <= pd.Timestamp('2020-03-07'):\n",
    "            return '2019/2020'\n",
    "        else:\n",
    "            return '2025/2026'\n",
    "\n",
    "    df['Season'] = df['Date'].apply(get_season)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering 2: add rolling goal-related overall performance columns(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_rolling_perf(df, rolling_window):\n",
    "    # Copy and sort dataset chronologically\n",
    "    df_raw = df.copy()\n",
    "    df_raw = df_raw.sort_values('Date').reset_index(drop=True)\n",
    "    df_raw['rowid'] = df_raw.index  # Unique ID to merge later\n",
    "\n",
    "    # Home team features\n",
    "    home_df = df_raw[['rowid', 'Date', 'HomeTeam', 'FTR', 'HS', 'HST', 'HC', 'HF', 'HY', 'HR', 'FTHG', 'FTAG']].copy()\n",
    "    home_df['team'] = home_df['HomeTeam']\n",
    "    home_df['side'] = 'home'\n",
    "    home_df['points'] = home_df['FTR'].map({'H': 3, 'D': 1, 'A': 0})\n",
    "    home_df = home_df.rename(columns={\n",
    "        'HS': 'shots', 'HST': 'shots_on_target',\n",
    "        'HC': 'corners', 'HF': 'fouls',\n",
    "        'HY': 'yellow_cards', 'HR': 'red_cards',\n",
    "        'FTHG': 'goals_scored', 'FTAG': 'goals_conceded'\n",
    "    })\n",
    "\n",
    "    # Away team features\n",
    "    away_df = df_raw[['rowid', 'Date', 'AwayTeam', 'FTR', 'AS', 'AST', 'AC', 'AF', 'AY', 'AR', 'FTAG', 'FTHG']].copy()\n",
    "    away_df['team'] = away_df['AwayTeam']\n",
    "    away_df['side'] = 'away'\n",
    "    away_df['points'] = away_df['FTR'].map({'A': 3, 'D': 1, 'H': 0})\n",
    "    away_df = away_df.rename(columns={\n",
    "        'AS': 'shots', 'AST': 'shots_on_target',\n",
    "        'AC': 'corners', 'AF': 'fouls',\n",
    "        'AY': 'yellow_cards', 'AR': 'red_cards',\n",
    "        'FTAG': 'goals_scored', 'FTHG': 'goals_conceded'\n",
    "    })\n",
    "\n",
    "    # Combine and sort\n",
    "    team_games = pd.concat([\n",
    "        home_df[['rowid', 'Date', 'team', 'side', 'points', 'shots', 'shots_on_target',\n",
    "                 'corners', 'fouls', 'yellow_cards', 'red_cards', 'goals_scored', 'goals_conceded']],\n",
    "        away_df[['rowid', 'Date', 'team', 'side', 'points', 'shots', 'shots_on_target',\n",
    "                 'corners', 'fouls', 'yellow_cards', 'red_cards', 'goals_scored', 'goals_conceded']]\n",
    "    ])\n",
    "    team_games = team_games.sort_values(by=['team', 'Date'])\n",
    "\n",
    "    # Weight setup\n",
    "    weights = np.arange(1, rolling_window + 1)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    def weighted_avg(series, weights_array):\n",
    "        w = weights_array[-len(series):]\n",
    "        return np.dot(series, w)\n",
    "\n",
    "    # Form Score (weighted Points)\n",
    "    team_games['form_score'] = (\n",
    "        team_games.groupby('team')['points']\n",
    "        .apply(lambda x: x.shift(1).rolling(window=rolling_window, min_periods=rolling_window)\n",
    "               .apply(lambda y: weighted_avg(y, weights), raw=True))\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Other stats (weighted Averages)\n",
    "    for col in ['shots', 'shots_on_target', 'corners', 'fouls',\n",
    "                'yellow_cards', 'red_cards', 'goals_scored', 'goals_conceded']:\n",
    "        team_games[f'avg_{col}'] = (\n",
    "            team_games.groupby('team')[col]\n",
    "            .apply(lambda x: x.shift(1).rolling(window=rolling_window, min_periods=rolling_window)\n",
    "                   .apply(lambda y: weighted_avg(y, weights), raw=True))\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    # Split home/away features\n",
    "    home_features = team_games[team_games['side'] == 'home'].copy()\n",
    "    away_features = team_games[team_games['side'] == 'away'].copy()\n",
    "\n",
    "    # Merge back to original data\n",
    "    df_enriched = df_raw.merge(home_features[[\n",
    "        'rowid', 'form_score', 'avg_shots', 'avg_shots_on_target', 'avg_corners',\n",
    "        'avg_fouls', 'avg_yellow_cards', 'avg_red_cards',\n",
    "        'avg_goals_scored', 'avg_goals_conceded'\n",
    "    ]], on='rowid', how='left').rename(columns={\n",
    "        'form_score': 'HomeTeam_FormScore',\n",
    "        'avg_shots': 'HomeTeam_AvgShots',\n",
    "        'avg_shots_on_target': 'HomeTeam_AvgShotsOnTarget',\n",
    "        'avg_corners': 'HomeTeam_AvgCorners',\n",
    "        'avg_fouls': 'HomeTeam_AvgFouls',\n",
    "        'avg_yellow_cards': 'HomeTeam_AvgYellowCards',\n",
    "        'avg_red_cards': 'HomeTeam_AvgRedCards',\n",
    "        'avg_goals_scored': 'HomeTeam_AvgGoalsScored',\n",
    "        'avg_goals_conceded': 'HomeTeam_AvgGoalsConceded'\n",
    "    })\n",
    "\n",
    "    df_enriched = df_enriched.merge(away_features[[\n",
    "        'rowid', 'form_score', 'avg_shots', 'avg_shots_on_target', 'avg_corners',\n",
    "        'avg_fouls', 'avg_yellow_cards', 'avg_red_cards',\n",
    "        'avg_goals_scored', 'avg_goals_conceded'\n",
    "    ]], on='rowid', how='left').rename(columns={\n",
    "        'form_score': 'AwayTeam_FormScore',\n",
    "        'avg_shots': 'AwayTeam_AvgShots',\n",
    "        'avg_shots_on_target': 'AwayTeam_AvgShotsOnTarget',\n",
    "        'avg_corners': 'AwayTeam_AvgCorners',\n",
    "        'avg_fouls': 'AwayTeam_AvgFouls',\n",
    "        'avg_yellow_cards': 'AwayTeam_AvgYellowCards',\n",
    "        'avg_red_cards': 'AwayTeam_AvgRedCards',\n",
    "        'avg_goals_scored': 'AwayTeam_AvgGoalsScored',\n",
    "        'avg_goals_conceded': 'AwayTeam_AvgGoalsConceded'\n",
    "    })\n",
    "\n",
    "    return df_enriched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering 3: add rolling goal-related head-to-head performance columns(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_h2h_rolling_perf(df, rolling_window):\n",
    "    df_raw = df.copy().sort_values('Date').reset_index(drop=True)\n",
    "    df_raw['rowid'] = df_raw.index\n",
    "\n",
    "    # Prepare match-level team/opponent data\n",
    "    home_df = df_raw[['rowid', 'Date', 'HomeTeam', 'AwayTeam', 'FTR',\n",
    "                      'HS', 'HST', 'HC', 'HF', 'HY', 'HR', 'FTHG', 'FTAG']].copy()\n",
    "    home_df['team'] = home_df['HomeTeam']\n",
    "    home_df['opponent'] = home_df['AwayTeam']\n",
    "    home_df['points'] = home_df['FTR'].map({'H': 3, 'D': 1, 'A': 0})\n",
    "    home_df = home_df.rename(columns={\n",
    "        'HS': 'shots', 'HST': 'shots_on_target',\n",
    "        'HC': 'corners', 'HF': 'fouls',\n",
    "        'HY': 'yellow_cards', 'HR': 'red_cards',\n",
    "        'FTHG': 'goals_scored', 'FTAG': 'goals_conceded'\n",
    "    })\n",
    "\n",
    "    away_df = df_raw[['rowid', 'Date', 'HomeTeam', 'AwayTeam', 'FTR',\n",
    "                      'AS', 'AST', 'AC', 'AF', 'AY', 'AR', 'FTAG', 'FTHG']].copy()\n",
    "    away_df['team'] = away_df['AwayTeam']\n",
    "    away_df['opponent'] = away_df['HomeTeam']\n",
    "    away_df['points'] = away_df['FTR'].map({'A': 3, 'D': 1, 'H': 0})\n",
    "    away_df = away_df.rename(columns={\n",
    "        'AS': 'shots', 'AST': 'shots_on_target',\n",
    "        'AC': 'corners', 'AF': 'fouls',\n",
    "        'AY': 'yellow_cards', 'AR': 'red_cards',\n",
    "        'FTAG': 'goals_scored', 'FTHG': 'goals_conceded'\n",
    "    })\n",
    "\n",
    "    # Combine\n",
    "    matches = pd.concat([home_df, away_df], ignore_index=True)\n",
    "    matches = matches.sort_values(['team', 'opponent', 'Date'])\n",
    "\n",
    "    # Rolling weights\n",
    "    weights = np.arange(1, rolling_window + 1) / np.arange(1, rolling_window + 1).sum()\n",
    "\n",
    "    def weighted_avg(series):\n",
    "        w = weights[-len(series):]\n",
    "        return np.dot(series, w)\n",
    "\n",
    "    # Compute H2H rolling stats for each (team, opponent) pair\n",
    "    stat_cols = ['points', 'shots', 'shots_on_target', 'corners',\n",
    "                 'fouls', 'yellow_cards', 'red_cards', 'goals_scored', 'goals_conceded']\n",
    "\n",
    "    for col in stat_cols:\n",
    "        matches[f'h2h_avg_{col}'] = (\n",
    "            matches.groupby(['team', 'opponent'])[col]\n",
    "            .apply(lambda x: x.shift(1).rolling(window=rolling_window, min_periods=rolling_window)\n",
    "                   .apply(weighted_avg, raw=True))\n",
    "            .reset_index(level=[0, 1], drop=True)\n",
    "        )\n",
    "\n",
    "    # Split back into home/away sets for merging\n",
    "    home_h2h = matches[matches['team'] == matches['HomeTeam']][\n",
    "        ['rowid'] + [f'h2h_avg_{c}' for c in stat_cols]\n",
    "    ]\n",
    "    away_h2h = matches[matches['team'] == matches['AwayTeam']][\n",
    "        ['rowid'] + [f'h2h_avg_{c}' for c in stat_cols]\n",
    "    ]\n",
    "\n",
    "    # Merge back into original df\n",
    "    df_enriched = df_raw.merge(\n",
    "        home_h2h, on='rowid', how='left'\n",
    "    ).rename(columns=lambda x: x.replace('h2h_avg_', 'HomeTeam_H2H_Avg'))\n",
    "\n",
    "    df_enriched = df_enriched.merge(\n",
    "        away_h2h, on='rowid', how='left'\n",
    "    ).rename(columns=lambda x: x.replace('h2h_avg_', 'AwayTeam_H2H_Avg'))\n",
    "\n",
    "    return df_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HomeTeam_FormScore', 'HomeTeam_AvgShots', 'HomeTeam_AvgShotsOnTarget', 'HomeTeam_AvgCorners', 'HomeTeam_AvgFouls', 'HomeTeam_AvgYellowCards', 'HomeTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'HomeTeam_AvgGoalsConceded', 'AwayTeam_FormScore', 'AwayTeam_AvgShots', 'AwayTeam_AvgShotsOnTarget', 'AwayTeam_AvgCorners', 'AwayTeam_AvgFouls', 'AwayTeam_AvgYellowCards', 'AwayTeam_AvgRedCards', 'AwayTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsConceded', 'HomeTeam_H2H_Avgpoints', 'HomeTeam_H2H_Avgshots', 'HomeTeam_H2H_Avgshots_on_target', 'HomeTeam_H2H_Avgcorners', 'HomeTeam_H2H_Avgfouls', 'HomeTeam_H2H_Avgyellow_cards', 'HomeTeam_H2H_Avgred_cards', 'HomeTeam_H2H_Avggoals_scored', 'HomeTeam_H2H_Avggoals_conceded', 'AwayTeam_H2H_Avgpoints', 'AwayTeam_H2H_Avgshots', 'AwayTeam_H2H_Avgshots_on_target', 'AwayTeam_H2H_Avgcorners', 'AwayTeam_H2H_Avgfouls', 'AwayTeam_H2H_Avgyellow_cards', 'AwayTeam_H2H_Avgred_cards', 'AwayTeam_H2H_Avggoals_scored', 'AwayTeam_H2H_Avggoals_conceded']\n"
     ]
    }
   ],
   "source": [
    "df = add_season(df_raw)\n",
    "df = add_rolling_perf(df, rolling_window=10)\n",
    "df = add_h2h_rolling_perf(df, rolling_window=3)\n",
    "df = df.iloc[:, -36:]\n",
    "df = df.dropna()\n",
    "list = df.columns.to_list()\n",
    "print(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train and test sets: ##\n",
    "*Note: Apply mannual preprocessing (i.e., feature engineering1 + feature engineering2) to dataset before feeding into sklearn/H2o/NN pipelines.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_sets(df_raw, rolling_window_overall, rolling_window_h2h, selected_features, test_seasons, train_seasons):\n",
    "    # Add 'Season' and rolling performance cols to dataset, drop null values\n",
    "    df = add_season(df_raw)\n",
    "    df = add_rolling_perf(df, rolling_window=rolling_window_overall)\n",
    "    df = add_h2h_rolling_perf(df, rolling_window=rolling_window_h2h)\n",
    "    df = df[selected_features + ['FTR'] +['Season']].dropna()\n",
    "\n",
    "    # Prepare train and test subsets\n",
    "    train_df = df[df['Season'].isin(train_seasons)]\n",
    "    train_df = train_df[selected_features + ['FTR']]\n",
    "    test_df = df[df['Season'].isin(test_seasons)]\n",
    "    test_df = test_df[selected_features + ['FTR']]\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['FTR']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test = test_df['FTR']\n",
    "\n",
    "    return train_df, test_df, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best single conventional ML model in sklearn (Metric: acc)\n",
    "*Loop all combinations of features & model*\n",
    "\n",
    "*Settings: train on 2019-2024 seasons, test on 2024-2025 season.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import softmax\n",
    "import itertools\n",
    "\n",
    "# Loop combination of features and model to find the best model:\n",
    "feature_groups = [['B365H', 'B365D', 'B365A'], # odds, mannually chosed\n",
    "                     ['HomeTeam_FormScore', 'AwayTeam_FormScore'],\n",
    "                     ['HomeTeam_AvgShots', 'AwayTeam_AvgShots'],\n",
    "                     ['HomeTeam_AvgShotsOnTarget', 'AwayTeam_AvgShotsOnTarget'],\n",
    "                     ['HomeTeam_AvgCorners', 'AwayTeam_AvgCorners'],\n",
    "                     ['HomeTeam_AvgFouls', 'AwayTeam_AvgFouls'],\n",
    "                     ['HomeTeam_AvgYellowCards', 'AwayTeam_AvgYellowCards'],\n",
    "                     ['HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards'],\n",
    "                     ['HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored'],\n",
    "                     ['HomeTeam_AvgGoalsConceded', 'AwayTeam_AvgGoalsConceded']]\n",
    "\n",
    "metric_ultra_max = 0\n",
    "model_ultra_max = \"Init\"\n",
    "features_ultra_max = []\n",
    "\n",
    "for r in range(1, len(feature_groups) + 1):\n",
    "    for combo in itertools.combinations(feature_groups, r):\n",
    "        # Prepare selected features\n",
    "        selected_features = ['HomeTeam', 'AwayTeam'] + [feature for group in combo for feature in group]\n",
    "        print(f\"📂 Selected features: {selected_features}\")\n",
    "\n",
    "        # Prepare train and test subsets\n",
    "        _, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "\n",
    "        # Make clear cat and num features in X for later sklearn pipeline\n",
    "        categorical_features = ['HomeTeam', 'AwayTeam']\n",
    "        numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "        # Prepare sklearn pipelines.\n",
    "        # Note that most models support one-hot, for models do not supporting sparse matrix (HistGradientBoosting, NaiveBayes, QDA), use ordinal encoding instead\n",
    "        # Note that imputer is useless if previous already dropna. Scaler is optional, we just applied it here.\n",
    "        numeric_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        # optional: PCA (proven to be useless, discard :( )\n",
    "        # numeric_pipeline = numeric_pipeline = Pipeline([\n",
    "        #     ('imputer', SimpleImputer(strategy='mean')),\n",
    "        #     ('scaler', StandardScaler()),\n",
    "        #     ('pca', PCA(n_components=0.95))\n",
    "        # ])\n",
    "        categorical_pipeline1 = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        categorical_pipeline2 = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ])\n",
    "        preprocessor1 = ColumnTransformer([\n",
    "            ('num', numeric_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline1, categorical_features)\n",
    "        ])\n",
    "        preprocessor2 = ColumnTransformer([\n",
    "            ('num', numeric_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline2, categorical_features)\n",
    "        ])\n",
    "        models_and_preprocessors = {\n",
    "            \"RandomForest\": [RandomForestClassifier(random_state=42), preprocessor1],\n",
    "            \"LogisticRegression\": [LogisticRegression(max_iter=1000, random_state=42), preprocessor1],\n",
    "            \"PassiveAggressiveClassifier\": [PassiveAggressiveClassifier(max_iter=1000, random_state=42), preprocessor1],\n",
    "            \"RidgeClassifier\": [RidgeClassifier(max_iter=1000), preprocessor1],\n",
    "            \"KNN\": [KNeighborsClassifier(), preprocessor1],\n",
    "            \"SVC\": [SVC(random_state=42), preprocessor1],\n",
    "            \"DecisionTree\": [DecisionTreeClassifier(random_state=42), preprocessor1],\n",
    "            \"GradientBoosting\": [GradientBoostingClassifier(random_state=42), preprocessor1],\n",
    "            \"HistGradientBoosting\": [HistGradientBoostingClassifier(random_state=42), preprocessor2],\n",
    "            \"AdaBoost\": [AdaBoostClassifier(random_state=42), preprocessor1],\n",
    "            \"ExtraTrees\": [ExtraTreesClassifier(random_state=42), preprocessor1],\n",
    "            \"NaiveBayes\": [GaussianNB(), preprocessor2],\n",
    "            \"MLP\": [MLPClassifier(max_iter=1000, random_state=42), preprocessor1],\n",
    "            \"QDA\": [QuadraticDiscriminantAnalysis(), preprocessor2],\n",
    "            \"LDA\": [LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto'), preprocessor2]\n",
    "        }\n",
    "\n",
    "        # Loop to train in different models and display selected features and accuracy\n",
    "        acc_max = 0\n",
    "        model_max = \"Init\"\n",
    "        for name, model_and_proprocessor in models_and_preprocessors.items():\n",
    "            model_pipeline = Pipeline([\n",
    "                ('preprocess', model_and_proprocessor[1]),\n",
    "                ('classifier', model_and_proprocessor[0])\n",
    "            ])\n",
    "            model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model_pipeline.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            print(f\"🎯 {name} Accuracy: {acc:.4f}\")\n",
    "            if acc > acc_max:\n",
    "                acc_max = acc\n",
    "                model_max = name\n",
    "\n",
    "        \n",
    "        # Update the best model in this selected features\n",
    "        if acc_max > metric_ultra_max:\n",
    "            metric_ultra_max = acc_max\n",
    "            model_ultra_max = model_max\n",
    "            features_ultra_max = selected_features\n",
    "        \n",
    "    print(f\"The best model until now is {model_ultra_max}.\\nThe accuracy is {metric_ultra_max}.\\nUsed features: {features_ultra_max}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "\n",
    "The best model is **LDA** with **05404** accuracy using the feature combination: ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored'].\n",
    "\n",
    "The best model without betting features is **RidgeClassifier** with **0.5292** accuracy using the feature combination: ['HomeTeam', 'AwayTeam', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgShots', 'AwayTeam_AvgShots', 'HomeTeam_AvgFouls', 'AwayTeam_AvgFouls', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards']\n",
    "\n",
    "See **sklearn_model_acc.txt** the records of all models' accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a way to improve the chosed model (tuning hyperparams?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Check and tune LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Check LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import pipeline_LDA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Check the draw of LDA\n",
    "print(\"😀Train LDA on 2019-2024, test on 2024-2025, check the test probabilities of Draw\")\n",
    "selected_features = ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored']\n",
    "_, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "pipe_LDA = pipeline_LDA()\n",
    "pipe_LDA.fit(X_train, y_train)\n",
    "\n",
    "probs = pipe_LDA.predict_proba(X_test)\n",
    "label_to_index = {label: idx for idx, label in enumerate(pipe_LDA.classes_)}\n",
    "probs_H = probs[:, label_to_index['H']]\n",
    "probs_D = probs[:, label_to_index['D']]\n",
    "probs_A = probs[:, label_to_index['A']]\n",
    "results_df = X_test.copy()\n",
    "results_df['Prob_H'] = probs_H\n",
    "results_df['Prob_D'] = probs_D\n",
    "results_df['Prob_A'] = probs_A\n",
    "display_cols = ['HomeTeam', 'AwayTeam', 'Prob_H', 'Prob_D', 'Prob_A']\n",
    "print(results_df[display_cols].to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "y_pred = pipe_LDA.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=pipe_LDA.classes_))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"🎯 Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: the draw is only predicted for 3 times, although the acc of predicted is high. Let's add Parity features and threshold tuning to solve this problem.\n",
    "\n",
    "*Class 'A' (Away Win): Precision: 0.44 → When the model predicts \"A\", it's right 44% of the time. Recall: 0.65 → Out of all actual \"A\" games, it correctly identified 65%.*\n",
    "\n",
    "*Class 'D' (Draw): Precision: 1.00 → The model only predicted \"D\" when it was correct — but this is misleading. Recall: 0.05 → It caught only 5% of actual draws! Terrible sensitivity.*\n",
    "\n",
    "*Class 'H' (Home Win): Precision: 0.59 → Not bad, 59% correct when predicting \"H\". Recall: 0.79 → It caught 79% of all actual home wins.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Tune with parity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "selected_features = ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored']\n",
    "_, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "X_train['FormDiff'] = abs(X_train['HomeTeam_FormScore'] - X_train['AwayTeam_FormScore'])\n",
    "X_test['FormDiff'] = abs(X_test['HomeTeam_FormScore'] - X_test['AwayTeam_FormScore'])\n",
    "X_train['RedcardsDiff'] = abs(X_train['HomeTeam_AvgRedCards'] - X_train['AwayTeam_AvgRedCards'])\n",
    "X_test['RedcardsDiff'] = abs(X_test['HomeTeam_AvgRedCards'] - X_test['AwayTeam_AvgRedCards'])\n",
    "X_train['GoalsscoredDiff'] = abs(X_train['HomeTeam_AvgGoalsScored'] - X_train['AwayTeam_AvgGoalsScored'])\n",
    "X_test['GoalsscoredDiff'] = abs(X_test['HomeTeam_AvgGoalsScored'] - X_test['AwayTeam_AvgGoalsScored'])\n",
    "\n",
    "added_feature_groups = [['FormDiff'], ['RedcardsDiff'],['GoalsscoredDiff']]\n",
    "\n",
    "for r in range(1, len(added_feature_groups) + 1):\n",
    "    for combo in itertools.combinations(added_feature_groups, r):\n",
    "        numerical_features = ['B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored'] + [feature for group in combo for feature in group]\n",
    "        print(f\"😀 numerical_features: {numerical_features}\")\n",
    "        pipe_LDA = Pipeline([\n",
    "            ('ColumnTransform', ColumnTransformer([\n",
    "                ('num', Pipeline([('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())]), numerical_features),\n",
    "                ('cat', Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))]), ['HomeTeam', 'AwayTeam'])\n",
    "            ])),\n",
    "            ('classifier', LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto'))\n",
    "        ])\n",
    "        pipe_LDA.fit(X_train, y_train)\n",
    "        probs = pipe_LDA.predict_proba(X_test)\n",
    "        labels = pipe_LDA.classes_\n",
    "        label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "        probs_H = probs[:, label_to_index['H']]\n",
    "        probs_D = probs[:, label_to_index['D']]\n",
    "        probs_A = probs[:, label_to_index['A']]\n",
    "        results_df = X_test.copy()\n",
    "        results_df['Prob_H'] = probs_H\n",
    "        results_df['Prob_D'] = probs_D\n",
    "        results_df['Prob_A'] = probs_A\n",
    "        display_cols = ['HomeTeam', 'AwayTeam', 'Prob_H', 'Prob_D', 'Prob_A']\n",
    "        print(results_df[display_cols].to_string(index=False, float_format='%.3f'))\n",
    "        y_pred = pipe_LDA.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"🎯 Accuracy: {acc:.4f}\")\n",
    "        print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: not big improvements, let's try threshold tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Tune with threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import pipeline_LDA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "selected_features = ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_FormScore', 'AwayTeam_FormScore', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored']\n",
    "_, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "pipe_LDA = pipeline_LDA()\n",
    "pipe_LDA.fit(X_train, y_train)\n",
    "\n",
    "probs = pipe_LDA.predict_proba(X_test)\n",
    "labels = pipe_LDA.classes_\n",
    "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "delta = 0.0000001\n",
    "y_pred = []\n",
    "for p in probs:\n",
    "    prob_H = p[label_to_index['H']]\n",
    "    prob_A = p[label_to_index['A']]\n",
    "    if abs(prob_H - prob_A) < delta:\n",
    "        y_pred.append('D')\n",
    "    else:\n",
    "        y_pred.append(labels[np.argmax(p)])\n",
    "\n",
    "# Step 5: Build results DataFrame\n",
    "results_df = X_test[['HomeTeam', 'AwayTeam']].copy()\n",
    "results_df['Prob_H'] = probs[:, label_to_index['H']]\n",
    "results_df['Prob_D'] = probs[:, label_to_index['D']]\n",
    "results_df['Prob_A'] = probs[:, label_to_index['A']]\n",
    "results_df['Predicted'] = y_pred\n",
    "results_df['Actual'] = y_test.values\n",
    "\n",
    "# Step 6: Print results and metrics\n",
    "print(results_df[['HomeTeam', 'AwayTeam', 'Prob_H', 'Prob_D', 'Prob_A', 'Predicted', 'Actual']].to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n🎯 Accuracy: {acc:.4f}\")\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: not big improvements, give up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonus: try this model in broer score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "print(y_test)\n",
    "lb = LabelBinarizer()\n",
    "y_true_bin = lb.fit_transform(y_test) # One-hot encoding for ['A', 'D', 'H']\n",
    "print(y_true_bin)\n",
    "\n",
    "# Match predicted probs to label order from binarizer\n",
    "labels = pipe_LDA.classes_\n",
    "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "probs_ordered = np.zeros_like(y_true_bin, dtype=float)\n",
    "for idx, label in enumerate(labels):\n",
    "    probs_ordered[:, idx] = probs[:, label_to_index[label]]\n",
    "print(probs_ordered)\n",
    "\n",
    "brier_multi = np.mean((probs_ordered - y_true_bin) ** 2)\n",
    "print(f\"📌 Overall Multi-class Brier Score: {brier_multi:.4f}\")\n",
    "\n",
    "# Per-Class Brier Scores\n",
    "brier_per_class = {}\n",
    "for label in labels:  # ['A', 'D', 'H']\n",
    "    y_true_binary = (y_test == label).astype(int)\n",
    "    y_prob = probs[:, label_to_index[label]]\n",
    "    brier_per_class[label] = brier_score_loss(y_true_binary, y_prob)\n",
    "\n",
    "print(\"📌 Per-class Brier Scores:\")\n",
    "for label, score in brier_per_class.items():\n",
    "    print(f\"{label}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best single conventional ML model in sklearn (Metric: brier score)\n",
    "*Loop all combinations of features & model*\n",
    "\n",
    "*Settings: train on 2019-2024 seasons, test on 2024-2025 season.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Selected features: ['HomeTeam_FormScore', 'AwayTeam_FormScore']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'HomeTeam'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:443\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     col_idx = \u001b[43mall_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers.Integral):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'HomeTeam'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model_and_proprocessor \u001b[38;5;129;01min\u001b[39;00m models_and_preprocessors.items():\n\u001b[32m    105\u001b[39m     model_pipeline = Pipeline([\n\u001b[32m    106\u001b[39m         (\u001b[33m'\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m'\u001b[39m, model_and_proprocessor[\u001b[32m1\u001b[39m]),\n\u001b[32m    107\u001b[39m         (\u001b[33m'\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m'\u001b[39m, model_and_proprocessor[\u001b[32m0\u001b[39m])\n\u001b[32m    108\u001b[39m     ])\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[43mmodel_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# Display brier score\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Not use calibration\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    try:\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m        probs = model_pipeline.predict_proba(X_test)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m \u001b[33;03m    brier = np.mean((probs_ordered - y_true_bin) ** 2)\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\pipeline.py:655\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n\u001b[32m    654\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\joblib\\memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:988\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformers()\n\u001b[32m    986\u001b[39m n_samples = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_remainder(X)\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:541\u001b[39m, in \u001b[36mColumnTransformer._validate_column_callables\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    539\u001b[39m         columns = columns(X)\n\u001b[32m    540\u001b[39m     all_columns.append(columns)\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     transformer_to_input_indices[name] = \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[38;5;28mself\u001b[39m._columns = all_columns\n\u001b[32m    544\u001b[39m \u001b[38;5;28mself\u001b[39m._transformer_to_input_indices = transformer_to_input_indices\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yinmi\\Becode_projects\\LGG-Thomas5\\env\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:451\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    448\u001b[39m         column_indices.append(col_idx)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mA given column is not a column of the dataframe\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[31mValueError\u001b[39m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import brier_score_loss\n",
    "# from scipy.special import softmax\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Loop combination of features and model to find the best model:\n",
    "feature_groups = [#['B365H', 'B365D', 'B365A'], # odds, mannually chosed\n",
    "                     ['HomeTeam_FormScore', 'AwayTeam_FormScore'],\n",
    "                     ['HomeTeam_AvgShots', 'AwayTeam_AvgShots'],\n",
    "                     ['HomeTeam_AvgShotsOnTarget', 'AwayTeam_AvgShotsOnTarget'],\n",
    "                     ['HomeTeam_AvgCorners', 'AwayTeam_AvgCorners'],\n",
    "                     ['HomeTeam_AvgFouls', 'AwayTeam_AvgFouls'],\n",
    "                     ['HomeTeam_AvgYellowCards', 'AwayTeam_AvgYellowCards'],\n",
    "                     ['HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards'],\n",
    "                     ['HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored'],\n",
    "                     ['HomeTeam_AvgGoalsConceded', 'AwayTeam_AvgGoalsConceded'],\n",
    "                     ['HomeTeam_H2H_Avgpoints', 'AwayTeam_H2H_Avgpoints'],\n",
    "                     ['HomeTeam_H2H_Avgshots', 'AwayTeam_H2H_Avgshots'],\n",
    "                     ['HomeTeam_H2H_Avgshots_on_target', 'AwayTeam_H2H_Avgshots_on_target'],\n",
    "                     ['HomeTeam_H2H_Avgcorners', 'AwayTeam_H2H_Avgcorners'],\n",
    "                     ['HomeTeam_H2H_Avgfouls', 'AwayTeam_H2H_Avgfouls'],\n",
    "                     ['HomeTeam_H2H_Avgyellow_cards', 'AwayTeam_H2H_Avgyellow_cards'],\n",
    "                     ['HomeTeam_H2H_Avgred_cards', 'AwayTeam_H2H_Avgred_cards'],\n",
    "                     ['HomeTeam_H2H_Avggoals_scored', 'AwayTeam_H2H_Avggoals_scored'],\n",
    "                     ['HomeTeam_H2H_Avggoals_conceded', 'AwayTeam_H2H_Avggoals_conceded']]\n",
    "\n",
    "brier_ultra_best = 1 # Initialize it with the worst, i.e., 1\n",
    "model_ultra_best = \"Init\"\n",
    "features_ultra_best = []\n",
    "\n",
    "for r in range(1, len(feature_groups) + 1):\n",
    "    for combo in itertools.combinations(feature_groups, r):\n",
    "        # Prepare selected features\n",
    "        selected_features = [feature for group in combo for feature in group] + ['HomeTeam', 'AwayTeam'] \n",
    "        print(f\"📂 Selected features: {selected_features}\")\n",
    "\n",
    "        # Prepare train and test subsets\n",
    "        _, _, X_train, y_train, X_test, y_test = prepare_train_test_sets(df_raw, rolling_window_overall=8, rolling_window_h2h=3, selected_features=selected_features, test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "\n",
    "        # Make clear cat and num features in X for later sklearn pipeline\n",
    "        categorical_features = ['HomeTeam', 'AwayTeam']\n",
    "        numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "        # Prepare sklearn pipelines.\n",
    "        # Note that most models support one-hot, for models do not supporting sparse matrix (HistGradientBoosting, NaiveBayes, QDA), use ordinal encoding instead\n",
    "        # Note that imputer is useless if previous already dropna. Scaler is optional, we just applied it here.\n",
    "        numeric_pipeline = Pipeline([\n",
    "            #('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        categorical_pipeline1 = Pipeline([\n",
    "            #('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        categorical_pipeline2 = Pipeline([\n",
    "            #('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ])\n",
    "        preprocessor1 = ColumnTransformer([\n",
    "            ('num', numeric_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline1, categorical_features)\n",
    "        ])\n",
    "        preprocessor2 = ColumnTransformer([\n",
    "            ('num', numeric_pipeline, numerical_features),\n",
    "            ('cat', categorical_pipeline2, categorical_features)\n",
    "        ])\n",
    "        models_and_preprocessors = {\n",
    "            \"RandomForest\": [RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_leaf=5, max_features='sqrt', random_state=42), preprocessor1],\n",
    "            \"LogisticRegression\": [LogisticRegression(max_iter=1000, C=1.0, solver='lbfgs', multi_class='multinomial', random_state=42), preprocessor1],\n",
    "            \"PassiveAggressiveClassifier\": [PassiveAggressiveClassifier(max_iter=1000, C=0.5, random_state=42), preprocessor1],\n",
    "            \"RidgeClassifier\": [RidgeClassifier(max_iter=1000, alpha=1.0), preprocessor1],\n",
    "            \"KNN\": [KNeighborsClassifier(n_neighbors=25, weights='distance'), preprocessor1],\n",
    "            \"SVC\": [SVC(C=1.0, kernel='rbf', gamma='scale', probability=True, random_state=42), preprocessor1],\n",
    "            \"DecisionTree\": [DecisionTreeClassifier(max_depth=10, min_samples_leaf=5, random_state=42), preprocessor1],\n",
    "            \"GradientBoosting\": [GradientBoostingClassifier(learning_rate=0.05, n_estimators=500, max_depth=3, min_samples_leaf=5, random_state=42), preprocessor1],\n",
    "            \"HistGradientBoosting\": [HistGradientBoostingClassifier(learning_rate=0.05, max_depth=6, min_samples_leaf=10, max_iter=1000, random_state=42), preprocessor2],\n",
    "            \"AdaBoost\": [AdaBoostClassifier(n_estimators=500, learning_rate=0.5, random_state=42), preprocessor1],\n",
    "            \"ExtraTrees\": [ExtraTreesClassifier(n_estimators=500, max_depth=None, min_samples_leaf=5, max_features='sqrt', random_state=42), preprocessor1],\n",
    "            \"NaiveBayes\": [GaussianNB(), preprocessor2],\n",
    "            \"MLP\": [MLPClassifier(max_iter=1000, hidden_layer_sizes=(100, 50), alpha=1e-4, learning_rate_init=1e-3, random_state=42), preprocessor1],\n",
    "            \"QDA\": [QuadraticDiscriminantAnalysis(reg_param=0.1), preprocessor2],\n",
    "            \"LDA\": [LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto'), preprocessor2]\n",
    "        }\n",
    "\n",
    "        # Loop to train in different models and display brier score\n",
    "        brier_best = 1\n",
    "        model_best = \"Init\"\n",
    "        for name, model_and_proprocessor in models_and_preprocessors.items():\n",
    "            model_pipeline = Pipeline([\n",
    "                ('preprocess', model_and_proprocessor[1]),\n",
    "                ('classifier', model_and_proprocessor[0])\n",
    "            ])\n",
    "            model_pipeline.fit(X_train, y_train)\n",
    "            # Display brier score\n",
    "            '''Not use calibration\n",
    "            try:\n",
    "                probs = model_pipeline.predict_proba(X_test)\n",
    "            except:\n",
    "                raw_scores = model_pipeline.decision_function(X_test)\n",
    "                if raw_scores.ndim == 1:\n",
    "                    raw_scores = np.vstack([-raw_scores, raw_scores]).T\n",
    "                probs = softmax(raw_scores, axis=1)\n",
    "            lb = LabelBinarizer()\n",
    "            y_true_bin = lb.fit_transform(y_test) # One-hot encoding for ['A', 'D', 'H']\n",
    "            labels = model_pipeline.classes_\n",
    "            label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "            probs_ordered = np.zeros_like(y_true_bin, dtype=float)\n",
    "            for idx, label in enumerate(labels):\n",
    "                probs_ordered[:, idx] = probs[:, label_to_index[label]]\n",
    "            brier = np.mean((probs_ordered - y_true_bin) ** 2)\n",
    "            '''\n",
    "\n",
    "            # Calibration for non-proba models\n",
    "            if name == \"NaiveBayes\" or name == \"QDA\" or name == \"LDA\":\n",
    "                y_proba = model_pipeline.predict_proba(X_test)\n",
    "            else:\n",
    "                tscv = TimeSeriesSplit(n_splits=5)\n",
    "                calibrated = CalibratedClassifierCV(\n",
    "                    estimator=model_pipeline,\n",
    "                    method='isotonic',\n",
    "                    cv=tscv\n",
    "                )\n",
    "                calibrated.fit(X_train, y_train)\n",
    "                y_proba = calibrated.predict_proba(X_test)\n",
    "            brier = brier_score_loss(y_test, y_proba)/3\n",
    "            print(f\"🎯 {name} brier score: {brier:.4f}\")\n",
    "\n",
    "            # Update the best brier score in this selected features\n",
    "            if brier < brier_best:\n",
    "                brier_best = brier\n",
    "                model_best = name\n",
    "\n",
    "        # Update the best model until now\n",
    "        if  brier_best < brier_ultra_best:\n",
    "            brier_ultra_best = brier_best\n",
    "            model_ultra_best = model_best\n",
    "            features_ultra_best = selected_features\n",
    "        print(f\"The best model until now is {model_ultra_best} with brier score {brier_ultra_best}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "\n",
    "1. With simple hyperparams\n",
    "\n",
    "    The best model is **LDA** with **0.59358586047** brier score using the feature combination: ['HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'HomeTeam_AvgFouls', 'AwayTeam_AvgFouls', 'HomeTeam_AvgYellowCards', 'AwayTeam_AvgYellowCards', 'HomeTeam_AvgRedCards', 'AwayTeam_AvgRedCards'].\n",
    "\n",
    "    The best model without betting features is **RidgeClassifier** with **0.6111** brier score using the feature combination: ['HomeTeam', 'AwayTeam', 'HomeTeam_AvgGoalsConceded', 'AwayTeam_AvgGoalsConceded']\n",
    "\n",
    "    See **sklearn_model_brier.txt** the records of all models' brier score.\n",
    "\n",
    "2. With more hyperparams\n",
    "\n",
    "    The best model without betting features is **ExtraTrees** with **0.60818908679** brier score using the feature combination: ['HomeTeam', 'AwayTeam', 'HomeTeam_AvgGoalsScored', 'AwayTeam_AvgGoalsScored', 'HomeTeam_AvgGoalsConceded', 'AwayTeam_AvgGoalsConceded']\n",
    "\n",
    "    See **sklearn_model_brier_morehyperparams.txt** the records of all models' brier score.\n",
    "\n",
    "3. With calibration for some models that does not have proba\n",
    "\n",
    "    The best model without betting features is **？** with **？** brier score using the feature combination: ['HomeTeam', 'AwayTeam', ]\n",
    "\n",
    "    See **sklearn_model_brier_calibration.txt** the records of all models' brier score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best mixed model in H2O AutoML\n",
    "*H2O automately selects features and decide mixed models*\n",
    "\n",
    "*Settings: train on 2019-2024 seasons, test on 2024-2025 season.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.automl import get_leaderboard\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initiate h2o with the train and test set\n",
    "h2o.init()\n",
    "train_df, test_df, _, _, _, _ = prepare_train_test_sets(df_raw, rolling_window=10, selected_features=\"all\", test_seasons=['2024/2025'], train_seasons=['2019/2020', '2020/2021', '2021/2022','2022/2023','2023/2024'])\n",
    "train_h2o = h2o.H2OFrame(train_df)\n",
    "test_h2o = h2o.H2OFrame(test_df)\n",
    "\n",
    "# Ensure H2o know the target is a classification problem\n",
    "target_col = 'FTR'\n",
    "train_h2o[target_col] = train_h2o[target_col].asfactor()\n",
    "test_h2o[target_col] = test_h2o[target_col].asfactor()\n",
    "\n",
    "# Ensure H2o know the categorial cols in predictor features is categorial\n",
    "for col in ['HomeTeam', 'AwayTeam']:\n",
    "    train_h2o[col] = train_h2o[col].asfactor()\n",
    "    test_h2o[col] = test_h2o[col].asfactor()\n",
    "\n",
    "# Set the param for automl and train it\n",
    "aml = H2OAutoML(\n",
    "    # max_models=50, # try maximum 50 models\n",
    "    max_runtime_secs=600, # limit total runtime to 600 seconds\n",
    "    balance_classes=True, # upsample the minority classes - \"Draw\"\n",
    "    sort_metric='logloss', # good for multi-class classification\n",
    "    nfolds=5, # use 5-fold cross-validation\n",
    "    stopping_metric='logloss', # early stopping based on log loss\n",
    "    stopping_rounds=30, # early stop after 30 rounds of no improvement\n",
    "    seed=42 # random state 42\n",
    ")\n",
    "aml.train(x=[col for col in train_h2o.columns if col != target_col], y=target_col, training_frame=train_h2o)\n",
    "\n",
    "# Get the best mixed model from the leaderboard\n",
    "lb = get_leaderboard(aml, extra_columns='ALL')\n",
    "top_models = lb.head(rows=1) # only need the top 1 model here\n",
    "model_id = top_models.as_data_frame().iloc[0]['model_id']\n",
    "model = h2o.get_model(model_id)\n",
    "\n",
    "# Show model and train-val info\n",
    "if model.algo == \"stackedensemble\":\n",
    "    print(f\"🖥️ {model.metalearner()}\")\n",
    "else:\n",
    "    print(f\"🖥️ {model.algo}\")\n",
    "    print(\"Info:\", model._model_json['output'])\n",
    "            \n",
    "# Display accuracy\n",
    "preds = model.predict(test_h2o).as_data_frame()['predict']\n",
    "true = test_h2o[target_col].as_data_frame()[target_col]\n",
    "acc = accuracy_score(true, preds)\n",
    "print(f\"🎯 Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "\n",
    "The best accuracy is **0.5037**, worse than single model, not adopted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
